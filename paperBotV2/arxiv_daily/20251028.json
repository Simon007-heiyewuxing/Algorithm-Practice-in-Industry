{
  "2510.23544v1": {
    "title": "LimRank: Less is More for Reasoning-Intensive Information Reranking",
    "url": "https://www.alphaxiv.org/abs/2510.23544v1",
    "arxiv_id": "2510.23544v1",
    "authors": "Tingyu Song, Yilun Zhao, Siyue Zhang, Chen Zhao, Arman Cohan",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-27 17:19:37",
    "ori_summary": "Existing approaches typically rely on large-scale fine-tuning to adapt LLMs for information reranking tasks, which is computationally expensive. In this work, we demonstrate that modern LLMs can be effectively adapted using only minimal, high-quality supervision. To enable this, we design LIMRANK-SYNTHESIZER, a reusable and open-source pipeline for generating diverse, challenging, and realistic reranking examples. Using this synthetic data, we fine-tune our reranker model, LIMRANK. We evaluate LIMRANK on two challenging benchmarks, i.e., BRIGHT for reasoning-intensive retrieval and FollowIR for instruction-following retrieval. Our experiments demonstrate that LIMRANK achieves competitive performance, while being trained on less than 5% of the data typically used in prior work. Further ablation studies demonstrate the effectiveness of LIMRANK-SYNTHESIZER and the strong generalization capabilities of LIMRANK across downstream tasks, including scientific literature search and retrieval-augmented generation for knowledge-intensive problem solving.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23224v1": {
    "title": "Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment",
    "url": "https://www.alphaxiv.org/abs/2510.23224v1",
    "arxiv_id": "2510.23224v1",
    "authors": "Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen",
    "categories": "cs.CV, cs.IR",
    "pub_date": "2025-10-27 11:22:28",
    "ori_summary": "The rapid digitization of histopathology slides has opened up new possibilities for computational tools in clinical and research workflows. Among these, content-based slide retrieval stands out, enabling pathologists to identify morphologically and semantically similar cases, thereby supporting precise diagnoses, enhancing consistency across observers, and assisting example-based education. However, effective retrieval of whole slide images (WSIs) remains challenging due to their gigapixel scale and the difficulty of capturing subtle semantic differences amid abundant irrelevant content. To overcome these challenges, we present PathSearch, a retrieval framework that unifies fine-grained attentive mosaic representations with global-wise slide embeddings aligned through vision-language contrastive learning. Trained on a corpus of 6,926 slide-report pairs, PathSearch captures both fine-grained morphological cues and high-level semantic patterns to enable accurate and flexible retrieval. The framework supports two key functionalities: (1) mosaic-based image-to-image retrieval, ensuring accurate and efficient slide research; and (2) multi-modal retrieval, where text queries can directly retrieve relevant slides. PathSearch was rigorously evaluated on four public pathology datasets and three in-house cohorts, covering tasks including anatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination, and grading across diverse organs such as breast, lung, kidney, liver, and stomach. External results show that PathSearch outperforms traditional image-to-image retrieval frameworks. A multi-center reader study further demonstrates that PathSearch improves diagnostic accuracy, boosts confidence, and enhances inter-observer agreement among pathologists in real clinical scenarios. These results establish PathSearch as a scalable and generalizable retrieval solution for digital pathology.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23104v1": {
    "title": "Leveraging Hierarchical Organization for Medical Multi-document Summarization",
    "url": "https://www.alphaxiv.org/abs/2510.23104v1",
    "arxiv_id": "2510.23104v1",
    "authors": "Yi-Li Hsu, Katelyn X. Mei, Lucy Lu Wang",
    "categories": "cs.CL, cs.AI, cs.IR",
    "pub_date": "2025-10-27 08:18:02",
    "ori_summary": "Medical multi-document summarization (MDS) is a complex task that requires effectively managing cross-document relationships. This paper investigates whether incorporating hierarchical structures in the inputs of MDS can improve a model's ability to organize and contextualize information across documents compared to traditional flat summarization methods. We investigate two ways of incorporating hierarchical organization across three large language models (LLMs), and conduct comprehensive evaluations of the resulting summaries using automated metrics, model-based metrics, and domain expert evaluation of preference, understandability, clarity, complexity, relevance, coverage, factuality, and coherence. Our results show that human experts prefer model-generated summaries over human-written summaries. Hierarchical approaches generally preserve factuality, coverage, and coherence of information, while also increasing human preference for summaries. Additionally, we examine whether simulated judgments from GPT-4 align with human judgments, finding higher agreement along more objective evaluation facets. Our findings demonstrate that hierarchical structures can improve the clarity of medical summaries generated by models while maintaining content coverage, providing a practical way to improve human preference for generated summaries.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23077v1": {
    "title": "Think before Recommendation: Autonomous Reasoning-enhanced Recommender",
    "url": "https://www.alphaxiv.org/abs/2510.23077v1",
    "arxiv_id": "2510.23077v1",
    "authors": "Xiaoyu Kong, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Jiancan Wu, Xiang Wang",
    "categories": "cs.IR, cs.AI",
    "pub_date": "2025-10-27 07:26:32",
    "ori_summary": "The core task of recommender systems is to learn user preferences from historical user-item interactions. With the rapid development of large language models (LLMs), recent research has explored leveraging the reasoning capabilities of LLMs to enhance rating prediction tasks. However, existing distillation-based methods suffer from limitations such as the teacher model's insufficient recommendation capability, costly and static supervision, and superficial transfer of reasoning ability. To address these issues, this paper proposes RecZero, a reinforcement learning (RL)-based recommendation paradigm that abandons the traditional multi-model and multi-stage distillation approach. Instead, RecZero trains a single LLM through pure RL to autonomously develop reasoning capabilities for rating prediction. RecZero consists of two key components: (1) \"Think-before-Recommendation\" prompt construction, which employs a structured reasoning template to guide the model in step-wise analysis of user interests, item features, and user-item compatibility; and (2) rule-based reward modeling, which adopts group relative policy optimization (GRPO) to compute rewards for reasoning trajectories and optimize the LLM. Additionally, the paper explores a hybrid paradigm, RecOne, which combines supervised fine-tuning with RL, initializing the model with cold-start reasoning samples and further optimizing it with RL. Experimental results demonstrate that RecZero and RecOne significantly outperform existing baseline methods on multiple benchmark datasets, validating the superiority of the RL paradigm in achieving autonomous reasoning-enhanced recommender systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23066v1": {
    "title": "Multi-Stage Field Extraction of Financial Documents with OCR and Compact Vision-Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23066v1",
    "arxiv_id": "2510.23066v1",
    "authors": "Yichao Jin, Yushuo Wang, Qishuai Zhong, Kent Chiu Jin-Chun, Kenneth Zhu Ke, Donald MacDonald",
    "categories": "cs.IR",
    "pub_date": "2025-10-27 06:56:08",
    "ori_summary": "Financial documents are essential sources of information for regulators, auditors, and financial institutions, particularly for assessing the wealth and compliance of Small and Medium-sized Businesses. However, SMB documents are often difficult to parse. They are rarely born digital and instead are distributed as scanned images that are none machine readable. The scans themselves are low in resolution, affected by skew or rotation, and often contain noisy backgrounds. These documents also tend to be heterogeneous, mixing narratives, tables, figures, and multilingual content within the same report. Such characteristics pose major challenges for automated information extraction, especially when relying on end to end large Vision Language Models, which are computationally expensive, sensitive to noise, and slow when applied to files with hundreds of pages. We propose a multistage pipeline that leverages traditional image processing models and OCR extraction, together with compact VLMs for structured field extraction of large-scale financial documents. Our approach begins with image pre-processing, including segmentation, orientation detection, and size normalization. Multilingual OCR is then applied to recover page-level text. Upon analyzing the text information, pages are retrieved for coherent sections. Finally, compact VLMs are operated within these narrowed-down scopes to extract structured financial indicators. Our approach is evaluated using an internal corpus of multi-lingual, scanned financial documents. The results demonstrate that compact VLMs, together with a multistage pipeline, achieves 8.8 times higher field level accuracy relative to directly feeding the whole document into large VLMs, only at 0.7 percent of the GPU cost and 92.6 percent less end-to-end service latency.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23018v1": {
    "title": "Improving Product Search Relevance with EAR-MP: A Solution for the CIKM 2025 AnalytiCup",
    "url": "https://www.alphaxiv.org/abs/2510.23018v1",
    "arxiv_id": "2510.23018v1",
    "authors": "JaeEun Lim, Soomin Kim, Jaeyong Seo, Iori Ono, Qimu Ran, Jae-woong Lee",
    "categories": "cs.IR",
    "pub_date": "2025-10-27 05:32:13",
    "ori_summary": "Multilingual e-commerce search is challenging due to linguistic diversity and the noise inherent in user-generated queries. This paper documents the solution employed by our team (EAR-MP) for the CIKM 2025 AnalytiCup, which addresses two core tasks: Query-Category (QC) relevance and Query-Item (QI) relevance. Our approach first normalizes the multilingual dataset by translating all text into English, then mitigates noise through extensive data cleaning and normalization. For model training, we build on DeBERTa-v3-large and improve performance with label smoothing, self-distillation, and dropout. In addition, we introduce task-specific upgrades, including hierarchical token injection for QC and a hybrid scoring mechanism for QI. Under constrained compute, our method achieves competitive results, attaining an F1 score of 0.8796 on QC and 0.8744 on QI. These findings underscore the importance of systematic data preprocessing and tailored training strategies for building robust, resource-efficient multilingual relevance systems.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22956v1": {
    "title": "Tagging-Augmented Generation: Assisting Language Models in Finding Intricate Knowledge In Long Contexts",
    "url": "https://www.alphaxiv.org/abs/2510.22956v1",
    "arxiv_id": "2510.22956v1",
    "authors": "Anwesan Pal, Karen Hovsepian, Tinghao Guo, Mengnan Zhao, Somendra Tripathi, Nikos Kanakaris, George Mihaila, Sumit Nigam",
    "categories": "cs.CL, cs.IR",
    "pub_date": "2025-10-27 03:23:25",
    "ori_summary": "Recent investigations into effective context lengths of modern flagship large language models (LLMs) have revealed major limitations in effective question answering (QA) and reasoning over long and complex contexts for even the largest and most impressive cadre of models. While approaches like retrieval-augmented generation (RAG) and chunk-based re-ranking attempt to mitigate this issue, they are sensitive to chunking, embedding and retrieval strategies and models, and furthermore, rely on extensive pre-processing, knowledge acquisition and indexing steps. In this paper, we propose Tagging-Augmented Generation (TAG), a lightweight data augmentation strategy that boosts LLM performance in long-context scenarios, without degrading and altering the integrity and composition of retrieved documents. We validate our hypothesis by augmenting two challenging and directly relevant question-answering benchmarks -- NoLima and NovelQA -- and show that tagging the context or even just adding tag definitions into QA prompts leads to consistent performance gains over the baseline -- up to 17% for 32K token contexts, and 2.9% in complex reasoning question-answering for multi-hop queries requiring knowledge across a wide span of text. Additional details are available at https://sites.google.com/view/tag-emnlp.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22942v1": {
    "title": "GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation",
    "url": "https://www.alphaxiv.org/abs/2510.22942v1",
    "arxiv_id": "2510.22942v1",
    "authors": "Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu",
    "categories": "cs.AI, cs.IR, H.3.3; I.2.6",
    "pub_date": "2025-10-27 02:56:08",
    "ori_summary": "Next Point-of-Interest (POI) recommendation is a critical task in modern Location-Based Social Networks (LBSNs), aiming to model the complex decision-making process of human mobility to provide personalized recommendations for a user's next check-in location. Existing POI recommendation models, predominantly based on Graph Neural Networks and sequential models, have been extensively studied. However, these models face a fundamental limitation: they struggle to simultaneously capture the inherent hierarchical structure of spatial choices and the dynamics and irregular shifts of user-specific temporal contexts. To overcome this limitation, we propose GTR-Mamba, a novel framework for cross-manifold conditioning and routing. GTR-Mamba leverages the distinct advantages of different mathematical spaces for different tasks: it models the static, tree-like preference hierarchies in hyperbolic geometry, while routing the dynamic sequence updates to a novel Mamba layer in the computationally stable and efficient Euclidean tangent space. This process is coordinated by a cross-manifold channel that fuses spatio-temporal information to explicitly steer the State Space Model (SSM), enabling flexible adaptation to contextual changes. Extensive experiments on three real-world datasets demonstrate that GTR-Mamba consistently outperforms state-of-the-art baseline models in next POI recommendation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22888v1": {
    "title": "MGFRec: Towards Reinforced Reasoning Recommendation with Multiple Groundings and Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.22888v1",
    "arxiv_id": "2510.22888v1",
    "authors": "Shihao Cai, Chongming Gao, Haoyan Liu, Wentao Shi, Jianshan Sun, Ruiming Tang, Fuli Feng",
    "categories": "cs.IR",
    "pub_date": "2025-10-27 00:41:07",
    "ori_summary": "The powerful reasoning and generative capabilities of large language models (LLMs) have inspired researchers to apply them to reasoning-based recommendation tasks, which require in-depth reasoning about user interests and the generation of recommended items. However, previous reasoning-based recommendation methods have typically performed inference within the language space alone, without incorporating the actual item space. This has led to over-interpreting user interests and deviating from real items. Towards this research gap, we propose performing multiple rounds of grounding during inference to help the LLM better understand the actual item space, which could ensure that its reasoning remains aligned with real items. Furthermore, we introduce a user agent that provides feedback during each grounding step, enabling the LLM to better recognize and adapt to user interests. Comprehensive experiments conducted on three Amazon review datasets demonstrate the effectiveness of incorporating multiple groundings and feedback. These findings underscore the critical importance of reasoning within the actual item space, rather than being confined to the language space, for recommendation tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23606v1": {
    "title": "Variational Masked Diffusion Models",
    "url": "https://www.alphaxiv.org/abs/2510.23606v1",
    "arxiv_id": "2510.23606v1",
    "authors": "Yichi Zhang, Alex Schwing, Zhizhen Zhao",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 17:59:57",
    "ori_summary": "Masked diffusion models have recently emerged as a flexible framework for discrete generative modeling. However, a key limitation of standard masked diffusion is its inability to effectively capture dependencies among tokens that are predicted concurrently, leading to degraded generation quality when dependencies among tokens are important. To explicitly model dependencies among tokens, we propose Variational Masked Diffusion (VMD), a framework that introduces latent variables into the masked diffusion process. Through controlled experiments on synthetic datasets, we demonstrate that VMD successfully learns dependencies that conventional masked diffusion fails to capture. We further validate the effectiveness of our approach on Sudoku puzzles and text datasets, where learning of dependencies among tokens improves global consistency. Across these domains, VMD enhances both generation quality and dependency awareness, highlighting the value of integrating variational inference into masked diffusion. Our code is available at: https://riccizz.github.io/VMD.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23596v1": {
    "title": "Think Twice: Branch-and-Rethink Reasoning Reward Model",
    "url": "https://www.alphaxiv.org/abs/2510.23596v1",
    "arxiv_id": "2510.23596v1",
    "authors": "Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 17:58:07",
    "ori_summary": "Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-oncescoringintofocused, second-lookreasoning, BR-RMreducesjudgmentdiffusionandimproves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains. The code and the model will be released soon.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23585v1": {
    "title": "Hope Speech Detection in Social Media English Corpora: Performance of Traditional and Transformer Models",
    "url": "https://www.alphaxiv.org/abs/2510.23585v1",
    "arxiv_id": "2510.23585v1",
    "authors": "Luis Ramos, Hiram Calvo, Olga Kolesnikova",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 17:53:40",
    "ori_summary": "The identification of hope speech has become a promised NLP task, considering the need to detect motivational expressions of agency and goal-directed behaviour on social media platforms. This proposal evaluates traditional machine learning models and fine-tuned transformers for a previously split hope speech dataset as train, development and test set. On development test, a linear-kernel SVM and logistic regression both reached a macro-F1 of 0.78; SVM with RBF kernel reached 0.77, and Na\\\"ive Bayes hit 0.75. Transformer models delivered better results, the best model achieved weighted precision of 0.82, weighted recall of 0.80, weighted F1 of 0.79, macro F1 of 0.79, and 0.80 accuracy. These results suggest that while optimally configured traditional machine learning models remain agile, transformer architectures detect some subtle semantics of hope to achieve higher precision and recall in hope speech detection, suggesting that larges transformers and LLMs could perform better in small datasets.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23564v1": {
    "title": "ReCode: Unify Plan and Action for Universal Granularity Control",
    "url": "https://www.alphaxiv.org/abs/2510.23564v1",
    "arxiv_id": "2510.23564v1",
    "authors": "Zhaoyang Yu, Jiayi Zhang, Huixue Su, Yufan Zhao, Yifan Wu, Mingyi Deng, Jinyu Xiang, Yizhang Lin, Lingxiao Tang, Yingchao Li, Yuyu Luo, Bang Liu, Chenglin Wu",
    "categories": "cs.AI, cs.CL, cs.LG",
    "pub_date": "2025-10-27 17:35:15",
    "ori_summary": "Real-world tasks require decisions at varying granularities, and humans excel at this by leveraging a unified cognitive representation where planning is fundamentally understood as a high-level form of action. However, current Large Language Model (LLM)-based agents lack this crucial capability to operate fluidly across decision granularities. This limitation stems from existing paradigms that enforce a rigid separation between high-level planning and low-level action, which impairs dynamic adaptability and limits generalization. We propose ReCode (Recursive Code Generation), a novel paradigm that addresses this limitation by unifying planning and action within a single code representation. In this representation, ReCode treats high-level plans as abstract placeholder functions, which the agent then recursively decomposes into finer-grained sub-functions until reaching primitive actions. This recursive approach dissolves the rigid boundary between plan and action, enabling the agent to dynamically control its decision granularity. Furthermore, the recursive structure inherently generates rich, multi-granularity training data, enabling models to learn hierarchical decision-making processes. Extensive experiments show ReCode significantly surpasses advanced baselines in inference performance and demonstrates exceptional data efficiency in training, validating our core insight that unifying planning and action through recursive code generation is a powerful and effective approach to achieving universal granularity control. The code is available at https://github.com/FoundationAgents/ReCode.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23558v1": {
    "title": "ISA-Bench: Benchmarking Instruction Sensitivity for Large Audio Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23558v1",
    "arxiv_id": "2510.23558v1",
    "authors": "Bohan Li, Wenbin Huang, Yuhang Qiu, Yiwei Guo, Hankun Wang, Zhihan Li, Jing Peng, Ziyang Ma, Xie Chen, Kai Yu",
    "categories": "cs.SD, cs.CL, eess.AS",
    "pub_date": "2025-10-27 17:31:25",
    "ori_summary": "Large Audio Language Models (LALMs), which couple acoustic perception with large language models (LLMs) to extract and understand diverse information from audio, have attracted intense interest from both academic and industrial communities. However, existing LALMs are highly sensitive to how instructions are phrased, affecting both (i) instruction-following rates and (ii) task performance. Yet, no existing benchmarks offer a systematic and comprehensive evaluation of this sensitivity. We introduce ISA-Bench, a dynamic benchmark evaluating instruction sensitivity for LALMs along three axes: instruction description, output format, and task composition. We assess recent open-source and proprietary LALMs using ISA-Bench, profiling both compliance and accuracy under controlled instruction variations. Experimental results reveal that even state-of-the-art LALMs suffer significant instruction sensitivity, leading to degraded performance on fundamental audio understanding tasks. To mitigate this issue, we fine-tune Qwen2-Audio on a specifically constructed complex instruction-variant dataset, achieving a marked improvement in instruction-following performance. However, this also induces nontrivial catastrophic forgetting: the model loses some previously mastered task capabilities when exposed to new instruction styles. Our benchmark provides a standardized basis for assessing and improving instruction sensitivity in LALMs, underscoring the need for instruction-robust audio understanding in real-world pipelines.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23554v1": {
    "title": "A U-Net and Transformer Pipeline for Multilingual Image Translation",
    "url": "https://www.alphaxiv.org/abs/2510.23554v1",
    "arxiv_id": "2510.23554v1",
    "authors": "Siddharth Sahay, Radhika Agarwal",
    "categories": "cs.LG, cs.CL, cs.CV",
    "pub_date": "2025-10-27 17:28:55",
    "ori_summary": "This paper presents an end-to-end multilingual translation pipeline that integrates a custom U-Net for text detection, the Tesseract engine for text recognition, and a from-scratch sequence-to-sequence (Seq2Seq) Transformer for Neural Machine Translation (NMT). Our approach first utilizes a U-Net model, trained on a synthetic dataset , to accurately segment and detect text regions from an image. These detected regions are then processed by Tesseract to extract the source text. This extracted text is fed into a custom Transformer model trained from scratch on a multilingual parallel corpus spanning 5 languages. Unlike systems reliant on monolithic pre-trained models, our architecture emphasizes full customization and adaptability. The system is evaluated on its text detection accuracy, text recognition quality, and translation performance via BLEU scores. The complete pipeline demonstrates promising results, validating the viability of a custom-built system for translating text directly from images.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23538v1": {
    "title": "JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence",
    "url": "https://www.alphaxiv.org/abs/2510.23538v1",
    "arxiv_id": "2510.23538v1",
    "authors": "Qiushi Sun, Jingyang Gong, Yang Liu, Qiaosheng Chen, Lei Li, Kai Chen, Qipeng Guo, Ben Kao, Fei Yuan",
    "categories": "cs.AI, cs.CL, cs.CV, cs.SE",
    "pub_date": "2025-10-27 17:13:49",
    "ori_summary": "The scope of neural code intelligence is rapidly expanding beyond text-based source code to encompass the rich visual outputs that programs generate. This visual dimension is critical for advanced applications like flexible content generation and precise, program-driven editing of visualizations. However, progress has been impeded by the scarcity of high-quality multimodal code data, a bottleneck stemming from challenges in synthesis and quality assessment. To address these challenges, we make contributions from both a data and modeling perspective. We first introduce a complete synthesis toolkit that leverages reciprocal synergies between data modalities to efficiently produce a large-scale, high-quality corpus spanning from standard charts to complex interactive web UIs and code-driven animations. Leveraging this toolkit, we construct JanusCode-800K, the largest multimodal code corpus to date. This powers the training of our models, JanusCoder and JanusCoderV, which establish a visual-programmatic interface for generating code from textual instructions, visual inputs, or a combination of both. Our unified model is a departure from existing approaches that build specialized models for isolated tasks. Extensive experiments on both text-centric and vision-centric coding tasks demonstrate the superior performance of the JanusCoder series, with our 7B to 14B scale models approaching or even exceeding the performance of commercial models. Furthermore, extensive analysis provides key insights into harmonizing programmatic logic with its visual expression. Our code and checkpoints will are available at https://github.com/InternLM/JanusCoder.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23536v1": {
    "title": "IPQA: A Benchmark for Core Intent Identification in Personalized Question Answering",
    "url": "https://www.alphaxiv.org/abs/2510.23536v1",
    "arxiv_id": "2510.23536v1",
    "authors": "Jieyong Kim, Maryam Amirizaniani, Soojin Yoon, Dongha Lee",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 17:12:49",
    "ori_summary": "Intent identification serves as the foundation for generating appropriate responses in personalized question answering (PQA). However, existing benchmarks evaluate only response quality or retrieval performance without directly measuring intent identification capabilities. This gap is critical because without understanding which intents users prioritize, systems cannot generate responses satisfying individual information needs. To address this, we introduce the concept of core intents: intents users prioritize when selecting answers to satisfy their information needs. To evaluate these core intents, we propose IPQA, a benchmark for core Intent identification in Personalized Question Answering. Since users do not explicitly state their prioritized intents, we derive core intents from observable behavior patterns in answer selection, grounded in satisficing theory where users choose answers meeting their acceptance thresholds. We construct a dataset with various domains through systematic filtering, LLM-based annotation, and rigorous quality control combining automated verification with human validation. Experimental evaluations across state-of-the-art language models reveal that current systems struggle with core intent identification in personalized contexts. Models fail to identify core intents from user histories, with performance degrading as question complexity increases. The code and dataset will be made publicly available to facilitate future research in this direction.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23508v1": {
    "title": "M4FC: a Multimodal, Multilingual, Multicultural, Multitask Real-World Fact-Checking Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.23508v1",
    "arxiv_id": "2510.23508v1",
    "authors": "Jiahui Geng, Jonathan Tonglet, Iryna Gurevych",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 16:44:35",
    "ori_summary": "Existing real-world datasets for multimodal automated fact-checking have multiple limitations: they contain few instances, focus on only one or two languages and tasks, suffer from evidence leakage, or depend on external sets of news articles for sourcing true claims. To address these shortcomings, we introduce M4FC, a new real-world dataset comprising 4,982 images paired with 6,980 claims. The images, verified by professional fact-checkers from 22 organizations, represent diverse cultural and geographic contexts. Each claim is available in one or two out of ten languages. M4FC spans six multimodal fact-checking tasks: visual claim extraction, claimant intent prediction, fake detection, image contextualization, location verification, and verdict prediction. We provide baseline results for all tasks and analyze how combining intermediate tasks influence downstream verdict prediction performance. We make our dataset and code available.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23477v1": {
    "title": "MMTutorBench: The First Multimodal Benchmark for AI Math Tutoring",
    "url": "https://www.alphaxiv.org/abs/2510.23477v1",
    "arxiv_id": "2510.23477v1",
    "authors": "Tengchao Yang, Sichen Guo, Mengzhao Jia, Jiaming Su, Yuanyang Liu, Zhihan Zhang, Meng Jiang",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 16:11:49",
    "ori_summary": "Effective math tutoring requires not only solving problems but also diagnosing students' difficulties and guiding them step by step. While multimodal large language models (MLLMs) show promise, existing benchmarks largely overlook these tutoring skills. We introduce MMTutorBench, the first benchmark for AI math tutoring, consisting of 685 problems built around pedagogically significant key-steps. Each problem is paired with problem-specific rubrics that enable fine-grained evaluation across six dimensions, and structured into three tasks-Insight Discovery, Operation Formulation, and Operation Execution. We evaluate 12 leading MLLMs and find clear performance gaps between proprietary and open-source systems, substantial room compared to human tutors, and consistent trends across input variants: OCR pipelines degrade tutoring quality, few-shot prompting yields limited gains, and our rubric-based LLM-as-a-Judge proves highly reliable. These results highlight both the difficulty and diagnostic value of MMTutorBench for advancing AI tutoring.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23464v1": {
    "title": "Evaluating Large Language Models for Stance Detection on Financial Targets from SEC Filing Reports and Earnings Call Transcripts",
    "url": "https://www.alphaxiv.org/abs/2510.23464v1",
    "arxiv_id": "2510.23464v1",
    "authors": "Nikesh Gyawali, Doina Caragea, Alex Vasenkov, Cornelia Caragea",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 16:03:20",
    "ori_summary": "Financial narratives from U.S. Securities and Exchange Commission (SEC) filing reports and quarterly earnings call transcripts (ECTs) are very important for investors, auditors, and regulators. However, their length, financial jargon, and nuanced language make fine-grained analysis difficult. Prior sentiment analysis in the financial domain required a large, expensive labeled dataset, making the sentence-level stance towards specific financial targets challenging. In this work, we introduce a sentence-level corpus for stance detection focused on three core financial metrics: debt, earnings per share (EPS), and sales. The sentences were extracted from Form 10-K annual reports and ECTs, and labeled for stance (positive, negative, neutral) using the advanced ChatGPT-o3-pro model under rigorous human validation. Using this corpus, we conduct a systematic evaluation of modern large language models (LLMs) using zero-shot, few-shot, and Chain-of-Thought (CoT) prompting strategies. Our results show that few-shot with CoT prompting performs best compared to supervised baselines, and LLMs' performance varies across the SEC and ECT datasets. Our findings highlight the practical viability of leveraging LLMs for target-specific stance in the financial domain without requiring extensive labeled data.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23458v1": {
    "title": "BrowseConf: Confidence-Guided Test-Time Scaling for Web Agents",
    "url": "https://www.alphaxiv.org/abs/2510.23458v1",
    "arxiv_id": "2510.23458v1",
    "authors": "Litu Ou, Kuan Li, Huifeng Yin, Liwen Zhang, Zhongwang Zhang, Xixi Wu, Rui Ye, Zile Qiao, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 15:58:51",
    "ori_summary": "Confidence in LLMs is a useful indicator of model uncertainty and answer reliability. Existing work mainly focused on single-turn scenarios, while research on confidence in complex multi-turn interactions is limited. In this paper, we investigate whether LLM-based search agents have the ability to communicate their own confidence through verbalized confidence scores after long sequences of actions, a significantly more challenging task compared to outputting confidence in a single interaction. Experimenting on open-source agentic models, we first find that models exhibit much higher task accuracy at high confidence while having near-zero accuracy when confidence is low. Based on this observation, we propose Test-Time Scaling (TTS) methods that use confidence scores to determine answer quality, encourage the model to try again until reaching a satisfactory confidence level. Results show that our proposed methods significantly reduce token consumption while demonstrating competitive performance compared to baseline fixed budget TTS methods.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23451v1": {
    "title": "Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences",
    "url": "https://www.alphaxiv.org/abs/2510.23451v1",
    "arxiv_id": "2510.23451v1",
    "authors": "Zhuoran Jin, Hongbang Yuan, Kejian Zhu, Jiachun Li, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao",
    "categories": "cs.CL, cs.AI, cs.CV",
    "pub_date": "2025-10-27 15:53:20",
    "ori_summary": "Reward models (RMs) play a critical role in aligning AI behaviors with human preferences, yet they face two fundamental challenges: (1) Modality Imbalance, where most RMs are mainly focused on text and image modalities, offering limited support for video, audio, and other modalities; and (2) Preference Rigidity, where training on fixed binary preference pairs fails to capture the complexity and diversity of personalized preferences. To address the above challenges, we propose Omni-Reward, a step toward generalist omni-modal reward modeling with support for free-form preferences, consisting of: (1) Evaluation: We introduce Omni-RewardBench, the first omni-modal RM benchmark with free-form preferences, covering nine tasks across five modalities including text, image, video, audio, and 3D; (2) Data: We construct Omni-RewardData, a multimodal preference dataset comprising 248K general preference pairs and 69K instruction-tuning pairs for training generalist omni-modal RMs; (3) Model: We propose Omni-RewardModel, which includes both discriminative and generative RMs, and achieves strong performance on Omni-RewardBench as well as other widely used reward modeling benchmarks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23443v1": {
    "title": "A Neuro-Symbolic Multi-Agent Approach to Legal-Cybersecurity Knowledge Integration",
    "url": "https://www.alphaxiv.org/abs/2510.23443v1",
    "arxiv_id": "2510.23443v1",
    "authors": "Chiara Bonfanti, Alessandro Druetto, Cataldo Basile, Tharindu Ranasinghe, Marcos Zampieri",
    "categories": "cs.AI, cs.CL, cs.CR, cs.MA",
    "pub_date": "2025-10-27 15:46:02",
    "ori_summary": "The growing intersection of cybersecurity and law creates a complex information space where traditional legal research tools struggle to deal with nuanced connections between cases, statutes, and technical vulnerabilities. This knowledge divide hinders collaboration between legal experts and cybersecurity professionals. To address this important gap, this work provides a first step towards intelligent systems capable of navigating the increasingly intricate cyber-legal domain. We demonstrate promising initial results on multilingual tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23396v1": {
    "title": "EMTSF:Extraordinary Mixture of SOTA Models for Time Series Forecasting",
    "url": "https://www.alphaxiv.org/abs/2510.23396v1",
    "arxiv_id": "2510.23396v1",
    "authors": "Musleh Alharthi, Kaleel Mahmood, Sarosh Patel, Ausif Mahmood",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 14:55:30",
    "ori_summary": "The immense success of the Transformer architecture in Natural Language Processing has led to its adoption in Time Se ries Forecasting (TSF), where superior performance has been shown. However, a recent important paper questioned their effectiveness by demonstrating that a simple single layer linear model outperforms Transformer-based models. This was soon shown to be not as valid, by a better transformer-based model termed PatchTST. More re cently, TimeLLM demonstrated even better results by repurposing a Large Language Model (LLM) for the TSF domain. Again, a follow up paper challenged this by demonstrating that removing the LLM component or replacing it with a basic attention layer in fact yields better performance. One of the challenges in forecasting is the fact that TSF data favors the more recent past, and is sometimes subject to unpredictable events. Based upon these recent insights in TSF, we propose a strong Mixture of Experts (MoE) framework. Our method combines the state-of-the-art (SOTA) models including xLSTM, en hanced Linear, PatchTST, and minGRU, among others. This set of complimentary and diverse models for TSF are integrated in a Trans former based MoE gating network. Our proposed model outperforms all existing TSF models on standard benchmarks, surpassing even the latest approaches based on MoE frameworks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23395v1": {
    "title": "Detecting Religious Language in Climate Discourse",
    "url": "https://www.alphaxiv.org/abs/2510.23395v1",
    "arxiv_id": "2510.23395v1",
    "authors": "Evy Beijen, Pien Pieterse, Yusuf Ã‡elik, Willem Th. van Peursen, Sandjai Bhulai, Meike Morren",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 14:54:51",
    "ori_summary": "Religious language continues to permeate contemporary discourse, even in ostensibly secular domains such as environmental activism and climate change debates. This paper investigates how explicit and implicit forms of religious language appear in climate-related texts produced by secular and religious nongovernmental organizations (NGOs). We introduce a dual methodological approach: a rule-based model using a hierarchical tree of religious terms derived from ecotheology literature, and large language models (LLMs) operating in a zero-shot setting. Using a dataset of more than 880,000 sentences, we compare how these methods detect religious language and analyze points of agreement and divergence. The results show that the rule-based method consistently labels more sentences as religious than LLMs. These findings highlight not only the methodological challenges of computationally detecting religious language but also the broader tension over whether religious language should be defined by vocabulary alone or by contextual meaning. This study contributes to digital methods in religious studies by demonstrating both the potential and the limitations of approaches for analyzing how the sacred persists in climate discourse.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23358v1": {
    "title": "How AI Forecasts AI Jobs: Benchmarking LLM Predictions of Labor Market Changes",
    "url": "https://www.alphaxiv.org/abs/2510.23358v1",
    "arxiv_id": "2510.23358v1",
    "authors": "Sheri Osborn, Rohit Valecha, H. Raghav Rao, Dan Sass, Anthony Rios",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 14:08:27",
    "ori_summary": "Artificial intelligence is reshaping labor markets, yet we lack tools to systematically forecast its effects on employment. This paper introduces a benchmark for evaluating how well large language models (LLMs) can anticipate changes in job demand, especially in occupations affected by AI. Existing research has shown that LLMs can extract sentiment, summarize economic reports, and emulate forecaster behavior, but little work has assessed their use for forward-looking labor prediction. Our benchmark combines two complementary datasets: a high-frequency index of sector-level job postings in the United States, and a global dataset of projected occupational changes due to AI adoption. We format these data into forecasting tasks with clear temporal splits, minimizing the risk of information leakage. We then evaluate LLMs using multiple prompting strategies, comparing task-scaffolded, persona-driven, and hybrid approaches across model families. We assess both quantitative accuracy and qualitative consistency over time. Results show that structured task prompts consistently improve forecast stability, while persona prompts offer advantages on short-term trends. However, performance varies significantly across sectors and horizons, highlighting the need for domain-aware prompting and rigorous evaluation protocols. By releasing our benchmark, we aim to support future research on labor forecasting, prompt design, and LLM-based economic reasoning. This work contributes to a growing body of research on how LLMs interact with real-world economic data, and provides a reproducible testbed for studying the limits and opportunities of AI as a forecasting tool in the context of labor markets.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23341v1": {
    "title": "LightKGG: Simple and Efficient Knowledge Graph Generation from Textual Data",
    "url": "https://www.alphaxiv.org/abs/2510.23341v1",
    "arxiv_id": "2510.23341v1",
    "authors": "Teng Lin",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:55:13",
    "ori_summary": "The scarcity of high-quality knowledge graphs (KGs) remains a critical bottleneck for downstream AI applications, as existing extraction methods rely heavily on error-prone pattern-matching techniques or resource-intensive large language models (LLMs). While recent tools leverage LLMs to generate KGs, their computational demands limit accessibility for low-resource environments. Our paper introduces LightKGG, a novel framework that enables efficient KG extraction from textual data using small-scale language models (SLMs) through two key technical innovations: (1) Context-integrated Graph extraction integrates contextual information with nodes and edges into a unified graph structure, reducing the reliance on complex semantic processing while maintaining more key information; (2) Topology-enhanced relationship inference leverages the inherent topology of the extracted graph to efficiently infer relationships, enabling relationship discovery without relying on complex language understanding capabilities of LLMs. By enabling accurate KG construction with minimal hardware requirements, this work bridges the gap between automated knowledge extraction and practical deployment scenarios while introducing scientifically rigorous methods for optimizing SLM efficiency in structured NLP tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23340v1": {
    "title": "Planning Ahead with RSA: Efficient Signalling in Dynamic Environments by Projecting User Awareness across Future Timesteps",
    "url": "https://www.alphaxiv.org/abs/2510.23340v1",
    "arxiv_id": "2510.23340v1",
    "authors": "Anwesha Das, John Duff, JÃ¶rg Hoffmann, Vera Demberg",
    "categories": "cs.AI, cs.CL, cs.HC",
    "pub_date": "2025-10-27 13:54:54",
    "ori_summary": "Adaptive agent design offers a way to improve human-AI collaboration on time-sensitive tasks in rapidly changing environments. In such cases, to ensure the human maintains an accurate understanding of critical task elements, an assistive agent must not only identify the highest priority information but also estimate how and when this information can be communicated most effectively, given that human attention represents a zero-sum cognitive resource where focus on one message diminishes awareness of other or upcoming information. We introduce a theoretical framework for adaptive signalling which meets these challenges by using principles of rational communication, formalised as Bayesian reference resolution using the Rational Speech Act (RSA) modelling framework, to plan a sequence of messages which optimise timely alignment between user belief and a dynamic environment. The agent adapts message specificity and timing to the particulars of a user and scenario based on projections of how prior-guided interpretation of messages will influence attention to the interface and subsequent belief update, across several timesteps out to a fixed horizon. In a comparison to baseline methods, we show that this effectiveness depends crucially on combining multi-step planning with a realistic model of user awareness. As the first application of RSA for communication in a dynamic environment, and for human-AI interaction in general, we establish theoretical foundations for pragmatic communication in human-agent teams, highlighting how insights from cognitive science can be capitalised to inform the design of assistive agents.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23337v1": {
    "title": "BaZi-Based Character Simulation Benchmark: Evaluating AI on Temporal and Persona Reasoning",
    "url": "https://www.alphaxiv.org/abs/2510.23337v1",
    "arxiv_id": "2510.23337v1",
    "authors": "Siyuan Zheng, Pai Liu, Xi Chen, Jizheng Dong, Sihan Jia",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:51:13",
    "ori_summary": "Human-like virtual characters are crucial for games, storytelling, and virtual reality, yet current methods rely heavily on annotated data or handcrafted persona prompts, making it difficult to scale up and generate realistic, contextually coherent personas. We create the first QA dataset for BaZi-based persona reasoning, where real human experiences categorized into wealth, health, kinship, career, and relationships are represented as life-event questions and answers. Furthermore, we propose the first BaZi-LLM system that integrates symbolic reasoning with large language models to generate temporally dynamic and fine-grained virtual personas. Compared with mainstream LLMs such as DeepSeek-v3 and GPT-5-mini, our method achieves a 30.3%-62.6% accuracy improvement. In addition, when incorrect BaZi information is used, our model's accuracy drops by 20%-45%, showing the potential of culturally grounded symbolic-LLM integration for realistic character simulation.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23334v1": {
    "title": "Adaptive Blockwise Search: Inference-Time Alignment for Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23334v1",
    "arxiv_id": "2510.23334v1",
    "authors": "Mohammad Atif Quamar, Mohammad Areeb, Nishant Sharma, Ananth Shreekumar, Jonathan Rosenthal, Muslum Ozgur Ozmen, Mikhail Kuznetsov, Z. Berkay Celik",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 13:48:59",
    "ori_summary": "LLM alignment remains a critical challenge. Inference-time methods provide a flexible alternative to fine-tuning, but their uniform computational effort often yields suboptimal alignment. We hypothesize that for many alignment tasks, the initial tokens of a response are disproportionately more critical. To leverage this principle, we introduce AdaSearch, a novel blockwise search strategy. It adaptively allocates a fixed computational budget using a sampling schedule, focusing search effort on these critical tokens. We apply AdaSearch to sequential decoding and introduce its tree-search counterpart, AdaBeam. Our comprehensive evaluation across eight LLMs demonstrates that AdaSearch outperforms strong Best-of-N and fine-tuning baselines. Specifically, win-rates improve by over 10% for harmlessness generation, controlled sentiment generation, and for mathematical reasoning tasks relative to Best-of-N.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23320v1": {
    "title": "LibriConvo: Simulating Conversations from Read Literature for ASR and Diarization",
    "url": "https://www.alphaxiv.org/abs/2510.23320v1",
    "arxiv_id": "2510.23320v1",
    "authors": "MÃ¡tÃ© Gedeon, PÃ©ter Mihajlik",
    "categories": "eess.AS, cs.CL, cs.SD",
    "pub_date": "2025-10-27 13:35:22",
    "ori_summary": "We introduce LibriConvo, a simulated multi-speaker conversational dataset based on speaker-aware conversation simulation (SASC), designed to support training and evaluation of speaker diarization and automatic speech recognition (ASR) systems. Unlike prior resources that mostly rely on semantically disconnected utterances and implausible temporal gaps, LibriConvo ensures semantic coherence and realistic conversational timing. Our pipeline leverages CallHome with external VAD for reliable boundaries, applies compression to reduce unnaturally long silences, and organizes LibriTTS utterances by book to maintain contextual consistency. Acoustic realism is enhanced via a novel room impulse response selection procedure that ranks speaker-microphone configurations by spatial plausibility, balancing realism and diversity. The dataset comprises 240.1 hours across 1,496 dialogues with 830 unique speakers, split in a speaker-disjoint manner for robust evaluation. Baselines show that the sortformer model outperforms the pyannote pipeline in diarization, while a fine-tuned Fast Conformer-CTC XLarge with Serialized Output Training achieves 7.29\\% WER for ASR, surpassing zero-shot Whisper-large-v3. LibriConvo provides a valuable resource for advancing multi-speaker speech processing research with realistic conversational dynamics and controlled experimental conditions.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23319v1": {
    "title": "Arabic Little STT: Arabic Children Speech Recognition Dataset",
    "url": "https://www.alphaxiv.org/abs/2510.23319v1",
    "arxiv_id": "2510.23319v1",
    "authors": "Mouhand Alkadri, Dania Desouki, Khloud Al Jallad",
    "categories": "cs.CL, cs.AI, cs.HC, cs.LG, cs.SD",
    "pub_date": "2025-10-27 13:30:54",
    "ori_summary": "The performance of Artificial Intelligence (AI) systems fundamentally depends on high-quality training data. However, low-resource languages like Arabic suffer from severe data scarcity. Moreover, the absence of child-specific speech corpora is an essential gap that poses significant challenges. To address this gap, we present our created dataset, Arabic Little STT, a dataset of Levantine Arabic child speech recorded in classrooms, containing 355 utterances from 288 children (ages 6 - 13). We further conduct a systematic assessment of Whisper, a state-of-the-art automatic speech recognition (ASR) model, on this dataset and compare its performance with adult Arabic benchmarks. Our evaluation across eight Whisper variants reveals that even the best-performing model (Large_v3) struggles significantly, achieving a 0.66 word error rate (WER) on child speech, starkly contrasting with its sub 0.20 WER on adult datasets. These results align with other research on English speech. Results highlight the critical need for dedicated child speech benchmarks and inclusive training data in ASR development. Emphasizing that such data must be governed by strict ethical and privacy frameworks to protect sensitive child information. We hope that this study provides an initial step for future work on equitable speech technologies for Arabic-speaking children. We hope that our publicly available dataset enrich the children's demographic representation in ASR datasets.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23284v1": {
    "title": "DCMM-SQL: Automated Data-Centric Pipeline and Multi-Model Collaboration Training for Text-to-SQL Model",
    "url": "https://www.alphaxiv.org/abs/2510.23284v1",
    "arxiv_id": "2510.23284v1",
    "authors": "Yuanzhen Xie, Liu Ye, Jiqun Chu, Mochi Gao, Hehuan Liu, Yunzhi Tan, Bo Hu, Zang Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:53:39",
    "ori_summary": "Text-to-SQL tasks have gained attractive improvements since the release of ChatGPT. Among them, agent-based frameworks have been widely used in this field. However, the impact of data-centric strategies on text-to-SQL tasks has rarely been explored. In this paper, we systemically design a fully automated data-centric pipeline for text-to-SQL tasks, including \\emph{adaptive data repair}, which can automatically find and fix errors in the training dataset; and \\emph{error data augmentation}, where we specifically diffuse and enhance erroneous data predicted by the initially trained models. Meanwhile, we propose a Multi-Model collaboration training schema, aiming to train multiple models with different augmented data, enabling them to possess distinct capabilities and work together to complement each other, because it has been found that the capability of a single fine-tuned model is very limited. Furthermore, we utilize an ensemble strategy to integrate the capabilities of multiple models to solve a multiple-choice question, aiming to further improve the accuracy of text-to-SQL tasks. The experiment results and ablation study have demonstrated the effectiveness of data-centric pipeline and Multi-Model(MM) interactive iterative strategies, achieving first place in lightweight text-to-SQL models (within 70B).",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23276v1": {
    "title": "A Cocktail-Party Benchmark: Multi-Modal dataset and Comparative Evaluation Results",
    "url": "https://www.alphaxiv.org/abs/2510.23276v1",
    "arxiv_id": "2510.23276v1",
    "authors": "Thai-Binh Nguyen, Katerina Zmolikova, Pingchuan Ma, Ngoc Quan Pham, Christian Fuegen, Alexander Waibel",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:36:43",
    "ori_summary": "We introduce the task of Multi-Modal Context-Aware Recognition (MCoRec) in the ninth CHiME Challenge, which addresses the cocktail-party problem of overlapping conversations in a single-room setting using audio, visual, and contextual cues. MCoRec captures natural multi-party conversations where the recordings focus on unscripted, casual group chats, leading to extreme speech overlap of up to 100% and highly fragmented conversational turns. The task requires systems to answer the question \"Who speaks when, what, and with whom?\" by jointly transcribing each speaker's speech and clustering them into their respective conversations from audio-visual recordings. Audio-only baselines exceed 100% word error rate, whereas incorporating visual cues yields substantial 50% improvements, highlighting the importance of multi-modality. In this manuscript, we present the motivation behind the task, outline the data collection process, and report the baseline systems developed for the MCoRec.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23272v1": {
    "title": "Code Aesthetics with Agentic Reward Feedback",
    "url": "https://www.alphaxiv.org/abs/2510.23272v1",
    "arxiv_id": "2510.23272v1",
    "authors": "Bang Xiao, Lingjie Jiang, Shaohan Huang, Tengchao Lv, Yupan Huang, Xun Wu, Lei Cui, Furu Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:32:33",
    "ori_summary": "Large Language Models (LLMs) have become valuable assistants for developers in code-related tasks. While LLMs excel at traditional programming tasks such as code generation and bug fixing, they struggle with visually-oriented coding tasks, often producing suboptimal aesthetics. In this paper, we introduce a new pipeline to enhance the aesthetic quality of LLM-generated code. We first construct AesCode-358K, a large-scale instruction-tuning dataset focused on code aesthetics. Next, we propose agentic reward feedback, a multi-agent system that evaluates executability, static aesthetics, and interactive aesthetics. Building on this, we develop GRPO-AR, which integrates these signals into the GRPO algorithm for joint optimization of functionality and code aesthetics. Finally, we develop OpenDesign, a benchmark for assessing code aesthetics. Experimental results show that combining supervised fine-tuning on AesCode-358K with reinforcement learning using agentic reward feedback significantly improves performance on OpenDesign and also enhances results on existing benchmarks such as PandasPlotBench. Notably, our AesCoder-4B surpasses GPT-4o and GPT-4.1, and achieves performance comparable to large open-source models with 480B-685B parameters, underscoring the effectiveness of our approach.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23271v1": {
    "title": "Mubeen AI: A Specialized Arabic Language Model for Heritage Preservation and User Intent Understanding",
    "url": "https://www.alphaxiv.org/abs/2510.23271v1",
    "arxiv_id": "2510.23271v1",
    "authors": "Mohammed Aljafari, Ismail Alturki, Ahmed Mori, Yehya Kadumi",
    "categories": "cs.CL, 68T50 (68T50 Natural language processing), I.2.7; I.2.6; I.2.0; H.3.3",
    "pub_date": "2025-10-27 12:29:27",
    "ori_summary": "Mubeen is a proprietary Arabic language model developed by MASARAT SA, optimized for deep understanding of Arabic linguistics, Islamic studies, and cultural heritage. Trained on an extensive collection of authentic Arabic sources significantly expanded by digitizing historical manuscripts via a proprietary Arabic OCR engine, the model incorporates seminal scholarly works in linguistics, jurisprudence, hadith, and Quranic exegesis, alongside thousands of academic theses and peer-reviewed research papers. Conditioned through a deep linguistic engineering framework, Mubeen masters not just the meaning but the eloquence of Arabic, enabling precise understanding across classical texts, contemporary writing, and regional dialects with focus on comprehending user intent and delivering accurate, contextually relevant responses. Unlike other Arabic models relying on translated English data that often fail in intent detection or retrieval-augmented generation (RAG), Mubeen uses native Arabic sources to ensure cultural authenticity and accuracy. Its core innovation is the Practical Closure Architecture, designed to solve the \"Utility Gap Crisis\" where factually correct answers fail to resolve users' core needs, forcing them into frustrating cycles of re-prompting. By prioritizing clarity and decisive guidance, Mubeen transforms from an information repository into a decisive guide, aligning with Saudi Vision 2030. The model's architecture combines deep heritage specialization with multi-disciplinary expert modules, enabling robust performance across both cultural preservation and general knowledge domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23252v1": {
    "title": "Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?",
    "url": "https://www.alphaxiv.org/abs/2510.23252v1",
    "arxiv_id": "2510.23252v1",
    "authors": "Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 12:14:52",
    "ori_summary": "Conventional research on speech recognition modeling relies on the canonical form for most low-resource languages while automatic speech recognition (ASR) for regional dialects is treated as a fine-tuning task. To investigate the effects of dialectal variations on ASR we develop a 78-hour annotated Bengali Speech-to-Text (STT) corpus named Ben-10. Investigation from linguistic and data-driven perspectives shows that speech foundation models struggle heavily in regional dialect ASR, both in zero-shot and fine-tuned settings. We observe that all deep learning methods struggle to model speech data under dialectal variations but dialect specific model training alleviates the issue. Our dataset also serves as a out of-distribution (OOD) resource for ASR modeling under constrained resources in ASR algorithms. The dataset and code developed for this project are publicly available",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23217v1": {
    "title": "Process Reward Models for Sentence-Level Verification of LVLM Radiology Reports",
    "url": "https://www.alphaxiv.org/abs/2510.23217v1",
    "arxiv_id": "2510.23217v1",
    "authors": "Alois Thomas, Maya Varma, Jean-Benoit Delbrouck, Curtis P. Langlotz",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 11:08:05",
    "ori_summary": "Automating radiology report generation with Large Vision-Language Models (LVLMs) holds great potential, yet these models often produce clinically critical hallucinations, posing serious risks. Existing hallucination detection methods frequently lack the necessary sentence-level granularity or robust generalization across different LVLM generators. We introduce a novel approach: a sentence-level Process Reward Model (PRM) adapted for this vision-language task. Our PRM predicts the factual correctness of each generated sentence, conditioned on clinical context and preceding text. When fine-tuned on MIMIC-CXR with weakly-supervised labels, a lightweight 0.5B-parameter PRM outperforms existing verification techniques, demonstrating, for instance, relative improvements of 7.5% in Matthews Correlation Coefficient and 1.8% in AUROC over strong white-box baselines on outputs from one LVLM. Unlike methods reliant on internal model states, our PRM demonstrates strong generalization to an unseen LVLM. We further show its practical utility: PRM scores effectively filter low-quality reports, improving F1-CheXbert scores by 4.5% (when discarding the worst 10% of reports). Moreover, when guiding a novel weighted best-of-N selection process on the MIMIC-CXR test set, our PRM show relative improvements in clinical metrics of 7.4% for F1-CheXbert and 0.6% for BERTScore. These results demonstrate that a lightweight, context-aware PRM provides a model-agnostic safety layer for clinical LVLMs without access to internal activations",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23198v1": {
    "title": "PTPP-Aware Adaptation Scaling Laws: Predicting Domain-Adaptation Performance at Unseen Pre-Training Budgets",
    "url": "https://www.alphaxiv.org/abs/2510.23198v1",
    "arxiv_id": "2510.23198v1",
    "authors": "Etienne Goffinet, Shane Bergsma, Avraham Sheinin, Natalia Vassilieva, Shaheer Muhammad, Preslav Nakov, Gurpreet Gosal",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 10:36:15",
    "ori_summary": "Continual pre-training (CPT) for domain adaptation must balance target-domain gains with stability on the base domain. Existing CPT scaling laws typically assume a fixed pre-training budget, which limits their ability to forecast adaptation outcomes for models trained at different tokens-per-parameter (PTPP). We present \\emph{PTPP-aware} adaptation scaling laws that make the pre-training budget an explicit variable, enabling accurate \\emph{prediction} of adaptation loss at unseen \\ptpp. On a multilingual setup (English/Arabic $\\rightarrow$ French), PTPP-aware formulations trained on early stages (\\ptpp{}=\\{15,31\\}) predict target loss at \\ptpp{}=279 and outperform a PTPP-agnostic \\dcpt{} transfer baseline on metrics (Huber-on-log, MAE$_\\mathrm{rel}$, calibration slope); full diagnostics (RMSE, MAPE) are in the appendix. Beyond forecasting, we show a practical use case: planning replay ratios and adaptation token budgets that satisfy target and forgetting constraints under compute limits.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23189v1": {
    "title": "DREaM: Drug-Drug Relation Extraction via Transfer Learning Method",
    "url": "https://www.alphaxiv.org/abs/2510.23189v1",
    "arxiv_id": "2510.23189v1",
    "authors": "Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-27 10:27:00",
    "ori_summary": "Relation extraction between drugs plays a crucial role in identifying drug drug interactions and predicting side effects. The advancement of machine learning methods in relation extraction, along with the development of large medical text databases, has enabled the low cost extraction of such relations compared to other approaches that typically require expert knowledge. However, to the best of our knowledge, there are limited datasets specifically designed for drug drug relation extraction currently available. Therefore, employing transfer learning becomes necessary to apply machine learning methods in this domain. In this study, we propose DREAM, a method that first employs a trained relation extraction model to discover relations between entities and then applies this model to a corpus of medical texts to construct an ontology of drug relationships. The extracted relations are subsequently validated using a large language model. Quantitative results indicate that the LLM agreed with 71 of the relations extracted from a subset of PubMed abstracts. Furthermore, our qualitative analysis indicates that this approach can uncover ambiguities in the medical domain, highlighting the challenges inherent in relation extraction in this field.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23182v1": {
    "title": "SI-Bench: Benchmarking Social Intelligence of Large Language Models in Human-to-Human Conversations",
    "url": "https://www.alphaxiv.org/abs/2510.23182v1",
    "arxiv_id": "2510.23182v1",
    "authors": "Shuai Huang, Wenxuan Zhao, Jun Gao",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 10:21:46",
    "ori_summary": "As large language models (LLMs) develop anthropomorphic abilities, they are increasingly being deployed as autonomous agents to interact with humans. However, evaluating their performance in realistic and complex social interactions remains a significant challenge. Most previous research built datasets through simulated agent-to-agent interactions, which fails to capture the authentic linguistic styles and relational dynamics found in real human conversations. To address this gap, we introduce SI-Bench, a novel benchmark designed to evaluate aspects of social intelligence in LLMs. Grounded in broad social science theories, SI-Bench contains 2,221 authentic multi-turn dialogues collected from a social networking application. We further selected a subset of 312 dialogues for manual annotation across 8 major models. The experiments show that SOTA models have surpassed the human expert in process reasoning under complex social situations, yet they still fall behind humans in reply quality. Moreover, introducing Chain-of-Thought (CoT) reasoning may degrade the performance of LLMs in social dialogue tasks. All datasets are openly available at https://github.com/SI-Bench/SI-Bench.git.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23169v1": {
    "title": "MATCH: Task-Driven Code Evaluation through Contrastive Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23169v1",
    "arxiv_id": "2510.23169v1",
    "authors": "Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro",
    "categories": "cs.CL, cs.SE",
    "pub_date": "2025-10-27 09:51:49",
    "ori_summary": "AI-based code generation is increasingly prevalent, with GitHub Copilot estimated to generate 46% of the code on GitHub. Accurately evaluating how well generated code aligns with developer intent remains a critical challenge. Traditional evaluation methods, such as unit tests, are often unscalable and costly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture code functionality, and metrics like CodeBERTScore require reference code, which is not always available. To address the gap in reference-free evaluation, with few alternatives such as ICE-Score, this paper introduces MATCH, a novel reference-free metric. MATCH uses Contrastive Learning to generate meaningful embeddings for code and natural language task descriptions, enabling similarity scoring that reflects how well generated code implements the task. We show that MATCH achieves stronger correlations with functional correctness and human preference than existing metrics across multiple programming languages.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23163v1": {
    "title": "Beyond Direct Generation: A Decomposed Approach to Well-Crafted Screenwriting with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.23163v1",
    "arxiv_id": "2510.23163v1",
    "authors": "Hang Lei, Shengyi Zong, Zhaoyan Li, Ziren Zhou, Hao Liu",
    "categories": "cs.CL, cs.AI, I.2.0",
    "pub_date": "2025-10-27 09:41:29",
    "ori_summary": "The screenplay serves as the foundation for television production, defining narrative structure, character development, and dialogue. While Large Language Models (LLMs) show great potential in creative writing, direct end-to-end generation approaches often fail to produce well-crafted screenplays. We argue this failure stems from forcing a single model to simultaneously master two disparate capabilities: creative narrative construction and rigid format adherence. The resulting outputs may mimic superficial style but lack the deep structural integrity and storytelling substance required for professional use. To enable LLMs to generate high-quality screenplays, we introduce Dual-Stage Refinement (DSR), a decomposed framework that decouples creative narrative generation from format conversion. The first stage transforms a brief outline into rich, novel-style prose. The second stage refines this narrative into a professionally formatted screenplay. This separation enables the model to specialize in one distinct capability at each stage. A key challenge in implementing DSR is the scarcity of paired outline-to-novel training data. We address this through hybrid data synthesis: reverse synthesis deconstructs existing screenplays into structured inputs, while forward synthesis leverages these inputs to generate high-quality narrative texts as training targets. Blind evaluations by professional screenwriters show that DSR achieves a 75% win rate against strong baselines like Gemini-2.5-Pro and reaches 82.7% of human-level performance. Our work demonstrates that decomposed generation architecture with tailored data synthesis effectively specializes LLMs in complex creative domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23160v1": {
    "title": "ENTP: Enhancing Low-Quality SFT Data via Neural-Symbolic Text Purge-Mix",
    "url": "https://www.alphaxiv.org/abs/2510.23160v1",
    "arxiv_id": "2510.23160v1",
    "authors": "Zile Yang, Ling Li, Na Di, Jinlong Pang, Yao Zhou, Hao Cheng, Bo Han, Jiaheng Wei",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 09:39:22",
    "ori_summary": "Supervised Fine-Tuning (SFT) adapts pre-trained Large Language Models (LLMs) to domain-specific instructions by training on a carefully curated subset of high-quality instruction-response pairs, typically drawn from a larger dataset that often contains many low-quality or noisy samples. However, existing quality-first paradigms often overlook valuable signals in discarded low-quality data and rely on imperfect quality filters. We introduce ENTP (Enhancing low-quality SFT data via Neural-symbolic Text Purge-Mix), a framework that revitalizes low-quality corpora through symbolic purification and neural reconstruction. The symbolic module identifies and prunes noisy samples based on statistical priors, while the neural component synthesizes enriched instruction-response pairs by leveraging latent representations and model knowledge. This neural-symbolic synergy enhances data informativeness and diversity. Experiments show that ENTP-augmented datasets, constructed exclusively from low-quality data, outperform 13 established data-selection baselines across five instruction-following benchmarks, and even surpass fine-tuning on the full original dataset (approximately 300K examples). Our results highlight the untapped potential of low-quality data and underscore the importance of intelligent purification and synthesis for efficient instruction alignment.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23142v1": {
    "title": "Rethinking GSPO: The Perplexity-Entropy Equivalence",
    "url": "https://www.alphaxiv.org/abs/2510.23142v1",
    "arxiv_id": "2510.23142v1",
    "authors": "Chi Liu",
    "categories": "cs.LG, cs.AI, cs.CL",
    "pub_date": "2025-10-27 09:19:10",
    "ori_summary": "We provide a new perspective on GSPO's length-normalized importance ratios by establishing their connection to information-theoretic quantities. We show that GSPO's sequence-level weight $s(\\theta) = (\\pi_\\theta/\\pi_{\\theta_{\\text{old}}})^{1/|y|}$ can be equivalently expressed as the inverse perplexity ratio $\\text{PPL}_{\\theta_{\\text{old}}}/\\text{PPL}_\\theta$ and as the exponential cross-entropy change $\\exp(\\Delta H)$. While the perplexity-entropy relationship follows from standard definitions, this observation provides a useful lens for understanding GSPO: the algorithm weights policy gradient updates by perplexity ratios, offering an information-theoretic interpretation of the importance weights. This perspective helps explain GSPO's empirical properties, including log-domain variance reduction through geometric averaging and stability in training mixture-of-experts models. We validate the mathematical equivalences and variance predictions through controlled experiments on mathematical reasoning tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23131v1": {
    "title": "Corpus Frequencies in Morphological Inflection: Do They Matter?",
    "url": "https://www.alphaxiv.org/abs/2510.23131v1",
    "arxiv_id": "2510.23131v1",
    "authors": "TomÃ¡Å¡ Sourada, Jana StrakovÃ¡",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 09:12:04",
    "ori_summary": "The traditional approach to morphological inflection (the task of modifying a base word (lemma) to express grammatical categories) has been, for decades, to consider lexical entries of lemma-tag-form triples uniformly, lacking any information about their frequency distribution. However, in production deployment, one might expect the user inputs to reflect a real-world distribution of frequencies in natural texts. With future deployment in mind, we explore the incorporation of corpus frequency information into the task of morphological inflection along three key dimensions during system development: (i) for train-dev-test split, we combine a lemma-disjoint approach, which evaluates the model's generalization capabilities, with a frequency-weighted strategy to better reflect the realistic distribution of items across different frequency bands in training and test sets; (ii) for evaluation, we complement the standard type accuracy (often referred to simply as accuracy), which treats all items equally regardless of frequency, with token accuracy, which assigns greater weight to frequent words and better approximates performance on running text; (iii) for training data sampling, we introduce a method novel in the context of inflection, frequency-aware training, which explicitly incorporates word frequency into the sampling process. We show that frequency-aware training outperforms uniform sampling in 26 out of 43 languages.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23123v1": {
    "title": "Beyond Higher Rank: Token-wise Input-Output Projections for Efficient Low-Rank Adaptation",
    "url": "https://www.alphaxiv.org/abs/2510.23123v1",
    "arxiv_id": "2510.23123v1",
    "authors": "Shiwei Li, Xiandi Luo, Haozhao Wang, Xing Tang, Ziqiang Cui, Dugang Liu, Yuhua Li, Xiuqiang He, Ruixuan Li",
    "categories": "cs.CL, cs.LG",
    "pub_date": "2025-10-27 08:57:24",
    "ori_summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method widely used in large language models (LLMs). LoRA essentially describes the projection of an input space into a low-dimensional output space, with the dimensionality determined by the LoRA rank. In standard LoRA, all input tokens share the same weights and undergo an identical input-output projection. This limits LoRA's ability to capture token-specific information due to the inherent semantic differences among tokens. To address this limitation, we propose Token-wise Projected Low-Rank Adaptation (TopLoRA), which dynamically adjusts LoRA weights according to the input token, thereby learning token-wise input-output projections in an end-to-end manner. Formally, the weights of TopLoRA can be expressed as $B\\Sigma_X A$, where $A$ and $B$ are low-rank matrices (as in standard LoRA), and $\\Sigma_X$ is a diagonal matrix generated from each input token $X$. Notably, TopLoRA does not increase the rank of LoRA weights but achieves more granular adaptation by learning token-wise LoRA weights (i.e., token-wise input-output projections). Extensive experiments across multiple models and datasets demonstrate that TopLoRA consistently outperforms LoRA and its variants. The code is available at https://github.com/Leopold1423/toplora-neurips25.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23114v1": {
    "title": "Flexing in 73 Languages: A Single Small Model for Multilingual Inflection",
    "url": "https://www.alphaxiv.org/abs/2510.23114v1",
    "arxiv_id": "2510.23114v1",
    "authors": "TomÃ¡Å¡ Sourada, Jana StrakovÃ¡",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 08:34:41",
    "ori_summary": "We present a compact, single-model approach to multilingual inflection, the task of generating inflected word forms from base lemmas to express grammatical categories. Our model, trained jointly on data from 73 languages, is lightweight, robust to unseen words, and outperforms monolingual baselines in most languages. This demonstrates the effectiveness of multilingual modeling for inflection and highlights its practical benefits: simplifying deployment by eliminating the need to manage and retrain dozens of separate monolingual models. In addition to the standard SIGMORPHON shared task benchmarks, we evaluate our monolingual and multilingual models on 73 Universal Dependencies (UD) treebanks, extracting lemma-tag-form triples and their frequency counts. To ensure realistic data splits, we introduce a novel frequency-weighted, lemma-disjoint train-dev-test resampling procedure. Our work addresses the lack of an open-source, general-purpose, multilingual morphological inflection system capable of handling unseen words across a wide range of languages, including Czech. All code is publicly released at: https://github.com/tomsouri/multilingual-inflection.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23090v1": {
    "title": "MAP4TS: A Multi-Aspect Prompting Framework for Time-Series Forecasting with Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23090v1",
    "arxiv_id": "2510.23090v1",
    "authors": "Suchan Lee, Jihoon Choi, Sohyeon Lee, Minseok Song, Bong-Gyu Jang, Hwanjo Yu, Soyeon Caren Han",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 07:51:54",
    "ori_summary": "Recent advances have investigated the use of pretrained large language models (LLMs) for time-series forecasting by aligning numerical inputs with LLM embedding spaces. However, existing multimodal approaches often overlook the distinct statistical properties and temporal dependencies that are fundamental to time-series data. To bridge this gap, we propose MAP4TS, a novel Multi-Aspect Prompting Framework that explicitly incorporates classical time-series analysis into the prompt design. Our framework introduces four specialized prompt components: a Global Domain Prompt that conveys dataset-level context, a Local Domain Prompt that encodes recent trends and series-specific behaviors, and a pair of Statistical and Temporal Prompts that embed handcrafted insights derived from autocorrelation (ACF), partial autocorrelation (PACF), and Fourier analysis. Multi-Aspect Prompts are combined with raw time-series embeddings and passed through a cross-modality alignment module to produce unified representations, which are then processed by an LLM and projected for final forecasting. Extensive experiments across eight diverse datasets show that MAP4TS consistently outperforms state-of-the-art LLM-based methods. Our ablation studies further reveal that prompt-aware designs significantly enhance performance stability and that GPT-2 backbones, when paired with structured prompts, outperform larger models like LLaMA in long-term forecasting tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23081v1": {
    "title": "A Survey on LLM Mid-training",
    "url": "https://www.alphaxiv.org/abs/2510.23081v1",
    "arxiv_id": "2510.23081v1",
    "authors": "Chengying Tu, Xuemiao Zhang, Rongxiang Weng, Rumei Li, Chen Zhang, Yang Bai, Hongfei Yan, Jingang Wang, Xunliang Cai",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 07:32:19",
    "ori_summary": "Recent advances in foundation models have highlighted the significant benefits of multi-stage training, with a particular emphasis on the emergence of mid-training as a vital stage that bridges pre-training and post-training. Mid-training is distinguished by its use of intermediate data and computational resources, systematically enhancing specified capabilities such as mathematics, coding, reasoning, and long-context extension, while maintaining foundational competencies. This survey provides a formal definition of mid-training for large language models (LLMs) and investigates optimization frameworks that encompass data curation, training strategies, and model architecture optimization. We analyze mainstream model implementations in the context of objective-driven interventions, illustrating how mid-training serves as a distinct and critical stage in the progressive development of LLM capabilities. By clarifying the unique contributions of mid-training, this survey offers a comprehensive taxonomy and actionable insights, supporting future research and innovation in the advancement of LLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23074v1": {
    "title": "Fast-MIA: Efficient and Scalable Membership Inference for LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.23074v1",
    "arxiv_id": "2510.23074v1",
    "authors": "Hiromu Takahashi, Shotaro Ishihara",
    "categories": "cs.CR, cs.CL",
    "pub_date": "2025-10-27 07:18:32",
    "ori_summary": "We propose Fast-MIA (https://github.com/Nikkei/fast-mia), a Python library for efficiently evaluating membership inference attacks (MIA) against Large Language Models (LLMs). MIA against LLMs has emerged as a crucial challenge due to growing concerns over copyright, security, and data privacy, and has attracted increasing research attention. However, the progress of this research is significantly hindered by two main obstacles: (1) the high computational cost of inference in LLMs, and (2) the lack of standardized and maintained implementations of MIA methods, which makes large-scale empirical comparison difficult. To address these challenges, our library provides fast batch inference and includes implementations of representative MIA methods under a unified evaluation framework. This library supports easy implementation of reproducible benchmarks with simple configuration and extensibility. We release Fast-MIA as an open-source (Apache License 2.0) tool to support scalable and transparent research on LLMs.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23070v1": {
    "title": "Quality-Aware Translation Tagging in Multilingual RAG system",
    "url": "https://www.alphaxiv.org/abs/2510.23070v1",
    "arxiv_id": "2510.23070v1",
    "authors": "Hoyeon Moon, Byeolhee Kim, Nikhil Verma",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 07:11:01",
    "ori_summary": "Multilingual Retrieval-Augmented Generation (mRAG) often retrieves English documents and translates them into the query language for low-resource settings. However, poor translation quality degrades response generation performance. Existing approaches either assume sufficient translation quality or utilize the rewriting method, which introduces factual distortion and hallucinations. To mitigate these problems, we propose Quality-Aware Translation Tagging in mRAG (QTT-RAG), which explicitly evaluates translation quality along three dimensions-semantic equivalence, grammatical accuracy, and naturalness&fluency-and attach these scores as metadata without altering the original content. We evaluate QTT-RAG against CrossRAG and DKM-RAG as baselines in two open-domain QA benchmarks (XORQA, MKQA) using six instruction-tuned LLMs ranging from 2.4B to 14B parameters, covering two low-resource languages (Korean and Finnish) and one high-resource language (Chinese). QTT-RAG outperforms the baselines by preserving factual integrity while enabling generator models to make informed decisions based on translation reliability. This approach allows for effective usage of cross-lingual documents in low-resource settings with limited native language documents, offering a practical and robust solution across multilingual domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23052v1": {
    "title": "Knocking-Heads Attention",
    "url": "https://www.alphaxiv.org/abs/2510.23052v1",
    "arxiv_id": "2510.23052v1",
    "authors": "Zhanchao Zhou, Xiaodong Chen, Haoxing Chen, Zhenzhong Lan, Jianguo Li",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 06:28:58",
    "ori_summary": "Multi-head attention (MHA) has become the cornerstone of modern large language models, enhancing representational capacity through parallel attention heads. However, increasing the number of heads inherently weakens individual head capacity, and existing attention mechanisms - whether standard MHA or its variants like grouped-query attention (GQA) and grouped-tied attention (GTA) - simply concatenate outputs from isolated heads without strong interaction. To address this limitation, we propose knocking-heads attention (KHA), which enables attention heads to \"knock\" on each other - facilitating cross-head feature-level interactions before the scaled dot-product attention. This is achieved by applying a shared, diagonally-initialized projection matrix across all heads. The diagonal initialization preserves head-specific specialization at the start of training while allowing the model to progressively learn integrated cross-head representations. KHA adds only minimal parameters and FLOPs and can be seamlessly integrated into MHA, GQA, GTA, and other attention variants. We validate KHA by training a 6.1B parameter MoE model (1.01B activated) on 1T high-quality tokens. Compared to baseline attention mechanisms, KHA brings superior and more stable training dynamics, achieving better performance across downstream tasks.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23038v1": {
    "title": "Incentivizing Agentic Reasoning in LLM Judges via Tool-Integrated Reinforcement Learning",
    "url": "https://www.alphaxiv.org/abs/2510.23038v1",
    "arxiv_id": "2510.23038v1",
    "authors": "Ran Xu, Jingjing Chen, Jiayu Ye, Yu Wu, Jun Yan, Carl Yang, Hongkun Yu",
    "categories": "cs.CL, cs.AI, cs.LG",
    "pub_date": "2025-10-27 06:03:37",
    "ori_summary": "Large Language Models (LLMs) are widely used as judges to evaluate response quality, providing a scalable alternative to human evaluation. However, most LLM judges operate solely on intrinsic text-based reasoning, limiting their ability to verify complex constraints or perform accurate computation. Motivated by the success of tool-integrated reasoning (TIR) in numerous tasks, we propose TIR-Judge, an end-to-end RL framework for training LLM judges that integrates a code executor for precise evaluation. TIR-Judge is built on three principles: (i) diverse training across verifiable and non-verifiable domains, (ii) flexible judgment formats (pointwise, pairwise, listwise), and (iii) iterative RL that bootstraps directly from the initial model without distillation. On seven public benchmarks, TIR-Judge surpasses strong reasoning-based judges by up to 6.4% (pointwise) and 7.7% (pairwise), and achieves listwise performance comparable to Claude-Opus-4 despite having only 8B parameters. Remarkably, TIR-Judge-Zero - trained entirely without distilled judge trajectories, matches the performance of distilled variants, demonstrating that tool-augmented judges can self-evolve through iterative reinforcement learning.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23027v1": {
    "title": "Towards Stable and Effective Reinforcement Learning for Mixture-of-Experts",
    "url": "https://www.alphaxiv.org/abs/2510.23027v1",
    "arxiv_id": "2510.23027v1",
    "authors": "Di Zhang, Xun Wu, Shaohan Huang, Yaru Hao, Li Dong, Zewen Chi, Zhifang Sui, Furu Wei",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 05:47:48",
    "ori_summary": "Recent advances in reinforcement learning (RL) have substantially improved the training of large-scale language models, leading to significant gains in generation quality and reasoning ability. However, most existing research focuses on dense models, while RL training for Mixture-of-Experts (MoE) architectures remains underexplored. To address the instability commonly observed in MoE training, we propose a novel router-aware approach to optimize importance sampling (IS) weights in off-policy RL. Specifically, we design a rescaling strategy guided by router logits, which effectively reduces gradient variance and mitigates training divergence. Experimental results demonstrate that our method significantly improves both the convergence stability and the final performance of MoE models, highlighting the potential of RL algorithmic innovations tailored to MoE architectures and providing a promising direction for efficient training of large-scale expert models.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23023v1": {
    "title": "UniAIDet: A Unified and Universal Benchmark for AI-Generated Image Content Detection and Localization",
    "url": "https://www.alphaxiv.org/abs/2510.23023v1",
    "arxiv_id": "2510.23023v1",
    "authors": "Huixuan Zhang, Xiaojun Wan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-27 05:37:23",
    "ori_summary": "With the rapid proliferation of image generative models, the authenticity of digital images has become a significant concern. While existing studies have proposed various methods for detecting AI-generated content, current benchmarks are limited in their coverage of diverse generative models and image categories, often overlooking end-to-end image editing and artistic images. To address these limitations, we introduce UniAIDet, a unified and comprehensive benchmark that includes both photographic and artistic images. UniAIDet covers a wide range of generative models, including text-to-image, image-to-image, image inpainting, image editing, and deepfake models. Using UniAIDet, we conduct a comprehensive evaluation of various detection methods and answer three key research questions regarding generalization capability and the relation between detection and localization. Our benchmark and analysis provide a robust foundation for future research.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23020v1": {
    "title": "M$^{3}$T2IBench: A Large-Scale Multi-Category, Multi-Instance, Multi-Relation Text-to-Image Benchmark",
    "url": "https://www.alphaxiv.org/abs/2510.23020v1",
    "arxiv_id": "2510.23020v1",
    "authors": "Huixuan Zhang, Xiaojun Wan",
    "categories": "cs.CV, cs.CL",
    "pub_date": "2025-10-27 05:32:50",
    "ori_summary": "Text-to-image models are known to struggle with generating images that perfectly align with textual prompts. Several previous studies have focused on evaluating image-text alignment in text-to-image generation. However, these evaluations either address overly simple scenarios, especially overlooking the difficulty of prompts with multiple different instances belonging to the same category, or they introduce metrics that do not correlate well with human evaluation. In this study, we introduce M$^3$T2IBench, a large-scale, multi-category, multi-instance, multi-relation along with an object-detection-based evaluation metric, $AlignScore$, which aligns closely with human evaluation. Our findings reveal that current open-source text-to-image models perform poorly on this challenging benchmark. Additionally, we propose the Revise-Then-Enforce approach to enhance image-text alignment. This training-free post-editing method demonstrates improvements in image-text alignment across a broad range of diffusion models. \\footnote{Our code and data has been released in supplementary material and will be made publicly available after the paper is accepted.}",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23011v1": {
    "title": "LangLingual: A Personalised, Exercise-oriented English Language Learning Tool Leveraging Large Language Models",
    "url": "https://www.alphaxiv.org/abs/2510.23011v1",
    "arxiv_id": "2510.23011v1",
    "authors": "Sammriddh Gupta, Sonit Singh, Aditya Joshi, Mira Kim",
    "categories": "cs.CL, cs.CY",
    "pub_date": "2025-10-27 05:11:07",
    "ori_summary": "Language educators strive to create a rich experience for learners, while they may be restricted in the extend of feedback and practice they can provide. We present the design and development of LangLingual, a conversational agent built using the LangChain framework and powered by Large Language Models. The system is specifically designed to provide real-time, grammar-focused feedback, generate context-aware language exercises and track learner proficiency over time. The paper discusses the architecture, implementation and evaluation of LangLingual in detail. The results indicate strong usability, positive learning outcomes and encouraging learner engagement.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23006v1": {
    "title": "Understanding In-Context Learning Beyond Transformers: An Investigation of State Space and Hybrid Architectures",
    "url": "https://www.alphaxiv.org/abs/2510.23006v1",
    "arxiv_id": "2510.23006v1",
    "authors": "Shenran Wang, Timothy Tin-Long Tse, Jian Zhu",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 04:49:01",
    "ori_summary": "We perform in-depth evaluations of in-context learning (ICL) on state-of-the-art transformer, state-space, and hybrid large language models over two categories of knowledge-based ICL tasks. Using a combination of behavioral probing and intervention-based methods, we have discovered that, while LLMs of different architectures can behave similarly in task performance, their internals could remain different. We discover that function vectors (FVs) responsible for ICL are primarily located in the self-attention and Mamba layers, and speculate that Mamba2 uses a different mechanism from FVs to perform ICL. FVs are more important for ICL involving parametric knowledge retrieval, but not for contextual knowledge understanding. Our work contributes to a more nuanced understanding across architectures and task types. Methodologically, our approach also highlights the importance of combining both behavioural and mechanistic analyses to investigate LLM capabilities.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22993v1": {
    "title": "Can Language Models Compose Skills In-Context?",
    "url": "https://www.alphaxiv.org/abs/2510.22993v1",
    "arxiv_id": "2510.22993v1",
    "authors": "Zidong Liu, Zhuoyan Xu, Zhenmei Shi, Yingyu Liang",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 04:18:59",
    "ori_summary": "Composing basic skills from simple tasks to accomplish composite tasks is crucial for modern intelligent systems. We investigate the in-context composition ability of language models to perform composite tasks that combine basic skills demonstrated in in-context examples. This is more challenging than the standard setting, where skills and their composition can be learned in training. We conduct systematic experiments on various representative open-source language models, utilizing linguistic and logical tasks designed to probe composition abilities. The results reveal that simple task examples can have a surprising negative impact on the performance, because the models generally struggle to recognize and assemble the skills correctly, even with Chain-of-Thought examples. Theoretical analysis further shows that it is crucial to align examples with the corresponding steps in the composition. This inspires a method for the probing tasks, whose improved performance provides positive support for our insights.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22968v1": {
    "title": "Measuring Teaching with LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.22968v1",
    "arxiv_id": "2510.22968v1",
    "authors": "Michael Hardy",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 03:42:04",
    "ori_summary": "Objective and scalable measurement of teaching quality is a persistent challenge in education. While Large Language Models (LLMs) offer potential, general-purpose models have struggled to reliably apply complex, authentic classroom observation instruments. This paper uses custom LLMs built on sentence-level embeddings, an architecture better suited for the long-form, interpretive nature of classroom transcripts than conventional subword tokenization. We systematically evaluate five different sentence embeddings under a data-efficient training regime designed to prevent overfitting. Our results demonstrate that these specialized models can achieve human-level and even super-human performance with expert human ratings above 0.65 and surpassing the average human-human rater correlation. Further, through analysis of annotation context windows, we find that more advanced models-those better aligned with human judgments-attribute a larger share of score variation to lesson-level features rather than isolated utterances, challenging the sufficiency of single-turn annotation paradigms. Finally, to assess external validity, we find that aggregate model scores align with teacher value-added measures, indicating they are capturing features relevant to student learning. However, this trend does not hold at the individual item level, suggesting that while the models learn useful signals, they have not yet achieved full generalization. This work establishes a viable and powerful new methodology for AI-driven instructional measurement, offering a path toward providing scalable, reliable, and valid feedback for educator development.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22967v1": {
    "title": "MAD-Fact: A Multi-Agent Debate Framework for Long-Form Factuality Evaluation in LLMs",
    "url": "https://www.alphaxiv.org/abs/2510.22967v1",
    "arxiv_id": "2510.22967v1",
    "authors": "Yucheng Ning, Xixun Lin, Fang Fang, Yanan Cao",
    "categories": "cs.CL, cs.AI",
    "pub_date": "2025-10-27 03:41:32",
    "ori_summary": "The widespread adoption of Large Language Models (LLMs) raises critical concerns about the factual accuracy of their outputs, especially in high-risk domains such as biomedicine, law, and education. Existing evaluation methods for short texts often fail on long-form content due to complex reasoning chains, intertwined perspectives, and cumulative information. To address this, we propose a systematic approach integrating large-scale long-form datasets, multi-agent verification mechanisms, and weighted evaluation metrics. We construct LongHalluQA, a Chinese long-form factuality dataset; and develop MAD-Fact, a debate-based multi-agent verification system. We introduce a fact importance hierarchy to capture the varying significance of claims in long-form texts. Experiments on two benchmarks show that larger LLMs generally maintain higher factual consistency, while domestic models excel on Chinese content. Our work provides a structured framework for evaluating and enhancing factual reliability in long-form LLM outputs, guiding their safe deployment in sensitive domains.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22954v1": {
    "title": "Artificial Hivemind: The Open-Ended Homogeneity of Language Models (and Beyond)",
    "url": "https://www.alphaxiv.org/abs/2510.22954v1",
    "arxiv_id": "2510.22954v1",
    "authors": "Liwei Jiang, Yuanjun Chai, Margaret Li, Mickel Liu, Raymond Fok, Nouha Dziri, Yulia Tsvetkov, Maarten Sap, Alon Albalak, Yejin Choi",
    "categories": "cs.CL",
    "pub_date": "2025-10-27 03:16:21",
    "ori_summary": "Language models (LMs) often struggle to generate diverse, human-like creative content, raising concerns about the long-term homogenization of human thought through repeated exposure to similar outputs. Yet scalable methods for evaluating LM output diversity remain limited, especially beyond narrow tasks such as random number or name generation, or beyond repeated sampling from a single model. We introduce Infinity-Chat, a large-scale dataset of 26K diverse, real-world, open-ended user queries that admit a wide range of plausible answers with no single ground truth. We introduce the first comprehensive taxonomy for characterizing the full spectrum of open-ended prompts posed to LMs, comprising 6 top-level categories (e.g., brainstorm & ideation) that further breaks down to 17 subcategories. Using Infinity-Chat, we present a large-scale study of mode collapse in LMs, revealing a pronounced Artificial Hivemind effect in open-ended generation of LMs, characterized by (1) intra-model repetition, where a single model consistently generates similar responses, and more so (2) inter-model homogeneity, where different models produce strikingly similar outputs. Infinity-Chat also includes 31,250 human annotations, across absolute ratings and pairwise preferences, with 25 independent human annotations per example. This enables studying collective and individual-specific human preferences in response to open-ended queries. Our findings show that LMs, reward models, and LM judges are less well calibrated to human ratings on model generations that elicit differing idiosyncratic annotator preferences, despite maintaining comparable overall quality. Overall, INFINITY-CHAT presents the first large-scale resource for systematically studying real-world open-ended queries to LMs, revealing critical insights to guide future research for mitigating long-term AI safety risks posed by the Artificial Hivemind.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22907v1": {
    "title": "Language Server CLI Empowers Language Agents with Process Rewards",
    "url": "https://www.alphaxiv.org/abs/2510.22907v1",
    "arxiv_id": "2510.22907v1",
    "authors": "Yifan Zhang, Lanser Contributors",
    "categories": "cs.CL, cs.AI, cs.PL, cs.SE",
    "pub_date": "2025-10-27 01:25:20",
    "ori_summary": "Large language models routinely hallucinate APIs and mislocalize edits, while language servers compute verified, IDE-grade facts about real code. We present Lanser-CLI, a CLI-first orchestration layer that pins and mediates a Language Server Protocol (LSP) server for coding agents and CI, exposing deterministic, replayable workflows. Our position is that language servers provide not only structural information (definitions, references, types, diagnostics) but also an actionable process reward: machine-checked, step-wise signals that align an agent's planning loop with program reality. In this work, Lanser-CLI contributes: (i) a robust addressing scheme beyond brittle \"file:line:col\" via a Selector DSL (symbolic, AST-path, and content-anchored selectors) with a principled relocation algorithm; (ii) deterministic Analysis Bundles that normalize Language Server responses and capture environment/capability metadata with stable content hashes; (iii) a safety envelope for mutating operations (rename, code actions) with preview, workspace jails, and Git-aware, transactional apply; and (iv) a process-reward functional derived from Language Server facts (diagnostic deltas, disambiguation confidence, and safe-apply checks) that is computable online and replayable offline. We formalize determinism under frozen snapshots and establish a monotonicity property for the process reward, making it suitable for process supervision and counterfactual analysis. Project Page: https://github.com/yifanzhang-pro/lanser-cli",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22904v1": {
    "title": "Modeling Political Discourse with Sentence-BERT and BERTopic",
    "url": "https://www.alphaxiv.org/abs/2510.22904v1",
    "arxiv_id": "2510.22904v1",
    "authors": "Margarida Mendonca, Alvaro Figueira",
    "categories": "cs.SI, cs.CL, cs.CY, 68T50, 91D30, I.2.7; H.3.1; J.4",
    "pub_date": "2025-10-27 01:19:42",
    "ori_summary": "Social media has reshaped political discourse, offering politicians a platform for direct engagement while reinforcing polarization and ideological divides. This study introduces a novel topic evolution framework that integrates BERTopic-based topic modeling with Moral Foundations Theory (MFT) to analyze the longevity and moral dimensions of political topics in Twitter activity during the 117th U.S. Congress. We propose a methodology for tracking dynamic topic shifts over time and measuring their association with moral values and quantifying topic persistence. Our findings reveal that while overarching themes remain stable, granular topics tend to dissolve rapidly, limiting their long-term influence. Moreover, moral foundations play a critical role in topic longevity, with Care and Loyalty dominating durable topics, while partisan differences manifest in distinct moral framing strategies. This work contributes to the field of social network analysis and computational political discourse by offering a scalable, interpretable approach to understanding moral-driven topic evolution on social media.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.22881v1": {
    "title": "Offline Preference Optimization via Maximum Marginal Likelihood Estimation",
    "url": "https://www.alphaxiv.org/abs/2510.22881v1",
    "arxiv_id": "2510.22881v1",
    "authors": "Saeed Najafi, Alona Fyshe",
    "categories": "cs.LG, cs.CL",
    "pub_date": "2025-10-27 00:15:57",
    "ori_summary": "Aligning Large Language Models (LLMs) with human preferences is crucial, but standard methods like Reinforcement Learning from Human Feedback (RLHF) are often complex and unstable. In this work, we propose a new, simpler approach that recasts alignment through the lens of Maximum Marginal Likelihood (MML) estimation. Our new MML based Preference Optimization (MMPO) maximizes the marginal log-likelihood of a preferred text output, using the preference pair as samples for approximation, and forgoes the need for both an explicit reward model and entropy maximization. We theoretically demonstrate that MMPO implicitly performs preference optimization, producing a weighted gradient that naturally up-weights chosen responses over rejected ones. Across models ranging from 135M to 8B parameters, we empirically show that MMPO: 1) is more stable with respect to the hyperparameter $\\beta$ compared to alternative baselines, and 2) achieves competitive or superior preference alignment while better preserving the base model's general language capabilities. Through a series of ablation experiments, we show that this improved performance is indeed attributable to MMPO's implicit preference optimization within the gradient updates.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  },
  "2510.23607v1": {
    "title": "Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations",
    "url": "https://www.alphaxiv.org/abs/2510.23607v1",
    "arxiv_id": "2510.23607v1",
    "authors": "Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao",
    "categories": "cs.CV",
    "pub_date": "2025-10-27 17:59:59",
    "ori_summary": "Humans learn abstract concepts through multisensory synergy, and once formed, such representations can often be recalled from a single modality. Inspired by this principle, we introduce Concerto, a minimalist simulation of human concept learning for spatial cognition, combining 3D intra-modal self-distillation with 2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns more coherent and informative spatial features, as demonstrated by zero-shot visualizations. It outperforms both standalone SOTA 2D and 3D self-supervised models by 14.2% and 4.8%, respectively, as well as their feature concatenation, in linear probing for 3D scene perception. With full fine-tuning, Concerto sets new SOTA results across multiple scene understanding benchmarks (e.g., 80.7% mIoU on ScanNet). We further present a variant of Concerto tailored for video-lifted point cloud spatial understanding, and a translator that linearly projects Concerto representations into CLIP's language space, enabling open-world perception. These results highlight that Concerto emerges spatial representations with superior fine-grained geometric and semantic consistency.",
    "summary": "",
    "translation": "",
    "relevance_score": 0,
    "reasoning": "",
    "rerank_relevance_score": 0,
    "rerank_reasoning": "",
    "is_filtered": true,
    "is_fine_ranked": false
  }
}